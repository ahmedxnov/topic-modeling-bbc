# BBC News Topic Modeling

A topic modeling project that discovers and identifies topics in BBC news articles using Latent Dirichlet Allocation (LDA).

## Project Overview
This project implements unsupervised topic modeling on the BBC news dataset using:

- **Advanced text preprocessing** with custom regex patterns, POS tagging, and lemmatization
- **Latent Dirichlet Allocation (LDA)** with optimized hyperparameters for topic discovery
- **Parallel processing architecture** for efficient document preprocessing and model training
- **Bag-of-Words vectorization** with vocabulary filtering and extremes handling
- **Interactive web interface** built with Streamlit for real-time topic prediction
- **Modular, production-ready code** with YAML configurations and comprehensive logging
- **Complete inference pipeline** supporting both batch and single document prediction

## Topics Identified
- ğŸ›ï¸ **Politics** (Topic 0): election, party, minister, tax, law, plan, issue, country, leader, case
- ğŸ’¼ **Business** (Topic 1): market, firm, sale, price, growth, rate, share, economy, business, deal  
- ğŸ¬ **Entertainment** (Topic 2): film, music, show, award, song, TV, star, director, band, movie
- ğŸ’» **Technology** (Topic 3): technology, phone, service, computer, user, network, firm, software, system, site
- âš½ **Sports** (Topic 4): game, player, team, match, side, club, title, minute, injury, season

## Features
- **Intelligent preprocessing pipeline** with POS-tag filtering (nouns only) and WordNet lemmatization
- **Efficient parallel processing** utilizing all CPU cores for scalable document preprocessing
- **Interactive Streamlit web app** with real-time topic prediction and probability visualization
- **Robust vocabulary filtering** reducing 10,014 initial terms to 837 meaningful words
- **Model persistence** with Gensim serialization for production deployment
- **Comprehensive logging system** with step-by-step pipeline progress tracking
- **Flexible YAML configuration** enabling hyperparameter tuning without code changes
- **Cross-platform compatibility** with automated directory creation and path handling

## Installation
### Prerequisites
- Python 3.8+
- conda or pip

### Setup
1. **Clone the repository:**
   ```bash
   git clone https://github.com/ahmedxnov/topic-modeling-bbc
   cd topic-modeling-bbc
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Download required NLTK data:**
   ```bash
   python -c "import nltk; nltk.download('punkt'); nltk.download('wordnet'); nltk.download('averaged_perceptron_tagger'); nltk.download('omw-1.4')"
   ```

## Project Structure

```
topic-modeling-bbc/
â”œâ”€â”€ config/                    # Configuration files
â”‚   â””â”€â”€ config.yaml           # LDA model hyperparameters
â”œâ”€â”€ dataset/                   # Dataset directory
â”‚   â””â”€â”€ Labeled BBC.csv       # BBC news articles dataset
â”œâ”€â”€ models/                    # Trained models (generated after training)
â”‚   â”œâ”€â”€ lda_model.gensim      # Trained LDA model
â”‚   â”œâ”€â”€ lda_model.gensim.expElogbeta.npy
â”‚   â”œâ”€â”€ lda_model.gensim.id2word
â”‚   â”œâ”€â”€ lda_model.gensim.state
â”‚   â””â”€â”€ vocabulary.dict       # Vocabulary dictionary
â”œâ”€â”€ scripts/                   # Executable scripts
â”‚   â”œâ”€â”€ app.py                # Streamlit web interface
â”‚   â”œâ”€â”€ inference.py          # Topic prediction for new documents
â”‚   â””â”€â”€ train.py              # Main training pipeline script
â”œâ”€â”€ src/                       # Source code modules
â”‚   â”œâ”€â”€ data/                 # Data processing modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ dataset_pipeline.py  # Data loading and parallel processing
â”‚   â”œâ”€â”€ utils/                # Utility modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ constants.py      # Text preprocessing patterns and NLP tools
â”‚   â”‚   â””â”€â”€ preprocessing.py  # Text preprocessing pipeline with POS filtering
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ .gitignore                # Git ignore file
â”œâ”€â”€ LICENSE                   # MIT License
â”œâ”€â”€ README.md                 # Project documentation
â””â”€â”€ requirements.txt          # Project dependencies
```

## Usage

### Training
```bash
python scripts/train.py
```

### Inference
```bash
python scripts/inference.py --new_data "your_document.txt"
```

### Web App
```bash
streamlit run scripts/app.py
```

Then open your browser to `http://localhost:8501`

## Dataset
- **Source**: [BBC Dataset](http://mlg.ucd.ie/datasets/bbc.html)
- **Documents**: 2,127 unique articles (duplicates removed)
- **Original size**: 2,225 documents
- **Categories**: Business, Entertainment, Politics, Sport, Tech

## Model Details
- **Algorithm**: Latent Dirichlet Allocation (LDA)
- **Implementation**: Gensim
- **Representation**: Bag of Words
- **Topics**: 5
- **Vocabulary**: 837 words (after filtering extremes)

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feat/amazing-feat`)
3. Commit your changes (`git commit -m 'Add amazing feat'`)
4. Push to the branch (`git push origin feat/amazing-feat`)
5. Open a Pull Request

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.